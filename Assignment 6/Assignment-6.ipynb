{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load IMDB dataset\n",
        "max_words = 10000   # vocab size\n",
        "max_len = 200       # max review length\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences to same length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-DMZq3HTBW-",
        "outputId": "ae55a1d0-908b-46db-92af-4430d1011faf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 758ms/step - accuracy: 0.7027 - loss: 0.5533 - val_accuracy: 0.8399 - val_loss: 0.3787\n",
            "Epoch 2/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 760ms/step - accuracy: 0.8369 - loss: 0.3762 - val_accuracy: 0.8538 - val_loss: 0.3500\n",
            "Epoch 3/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 853ms/step - accuracy: 0.8882 - loss: 0.2862 - val_accuracy: 0.8385 - val_loss: 0.3962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7db5e03ccd70>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"I absolutely loved this movie, it was fantastic!\",\n",
        "    \"The product broke after one day of use.\",\n",
        "    \"The movie was okay, nothing special but not terrible either.\"\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_texts)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len)\n",
        "predictions = model.predict(test_pad)\n",
        "\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    sentiment = \"Positive\" if pred > 0.65 else \"Negative\"\n",
        "    print(f\"{text} --> {sentiment} ({pred[0]:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVdnRzFcXa8C",
        "outputId": "d7a4aa1f-98a8-44f7-b2d6-80c03eaaddd1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "I absolutely loved this movie, it was fantastic! --> Positive (0.9028)\n",
            "The product broke after one day of use. --> Negative (0.5160)\n",
            "The movie was okay, nothing special but not terrible either. --> Negative (0.1406)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8cf29f2",
        "outputId": "77cdd70a-35aa-414f-ee47-7629e8e49e9e"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
        "\n",
        "tokenizer.word_index = word_index\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"I absolutely loved this movie, it was fantastic!\",\n",
        "    \"The product broke after one day of use.\",\n",
        "    \"The movie was okay, nothing special but not terrible either.\"\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_texts)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len)\n",
        "predictions = model.predict(test_pad)\n",
        "\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    sentiment = \"Positive\" if pred > 0.65 else \"Negative\"\n",
        "    print(f\"{text} --> {sentiment} ({pred[0]:.4f})\")\n"
      ],
      "metadata": {
        "id": "kAGb73NDX7rN",
        "outputId": "025f04d3-ce54-4f11-d533-529ed52f3a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "I absolutely loved this movie, it was fantastic! --> Positive (0.9028)\n",
            "The product broke after one day of use. --> Negative (0.5160)\n",
            "The movie was okay, nothing special but not terrible either. --> Negative (0.1406)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}